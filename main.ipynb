{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszAxEbZOA43"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyGzkc5ez3a"
      },
      "source": [
        "# Download e exploração inicial dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CuxJtGggAxGf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pubmedqa/pubmedqa.git\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = 'pubmedqa/data/ori_pqal.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_key = list(data.keys())[0]\n",
        "print(f\"\\nCampos disponíveis: {list(data[sample_key].keys())}\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Exploração de dados - PubMedQA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, key in enumerate(list(data.keys())[:3]):\n",
        "    item = data[key]\n",
        "\n",
        "    print(f\"\\nExemplo {i+1} | ID: {key}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Question: {item.get('QUESTION', 'N/A')}\")\n",
        "\n",
        "    context = \" \".join(item.get('CONTEXTS', []))\n",
        "    print(f\"Context: {context[:300]}...\")\n",
        "\n",
        "    print(f\"Labels: {item.get('LABELS', 'N/A')}\")\n",
        "    print(f\"Decision: {item.get('final_decision', 'N/A')}\")\n",
        "    print(f\"Answer: {item.get('LONG_ANSWER', 'N/A')[:200]}...\")\n",
        "    print(f\"Meshes: {item.get('MESHES', 'N/A')}\")\n",
        "    print(f\"Year: {item.get('YEAR', 'N/A')}\")\n",
        "    print(f\"Reasoning required pred: {item.get('reasoning_required_pred', 'N/A')}\")\n",
        "    print(f\"Reasoning free pred: {item.get('reasoning_free_pred', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n\\nTotal de registros: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFH2sIYarWe"
      },
      "source": [
        "# Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4fd06e94"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Normaliza e limpa texto\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def map_decision(decision):\n",
        "    decision = decision.lower()\n",
        "    if decision == \"yes\":\n",
        "        return \"SIM\"\n",
        "    elif decision == \"no\":\n",
        "        return \"NÃO\"\n",
        "    elif decision == \"maybe\":\n",
        "        return \"TALVEZ\"\n",
        "    return\n",
        "\n",
        "def preprocess_dataset(input_path, output_path):\n",
        "    \"\"\"Pré-processa o dataset original completo\"\"\"\n",
        "    print(f\"Carregando {input_path}...\")\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    for key, item in data.items():\n",
        "\n",
        "        # QUESTION\n",
        "        question = preprocess_text(item.get('QUESTION', ''))\n",
        "\n",
        "        # CONTEXTS (string ou lista)\n",
        "        context_raw = \" \".join(item.get('CONTEXTS', []))\n",
        "        context = preprocess_text(context_raw)\n",
        "\n",
        "        # DECISION\n",
        "        decision_raw = item.get('final_decision', 'N/A')\n",
        "        decision = map_decision(decision_raw)\n",
        "\n",
        "        # LONG_ANSWER\n",
        "        long_answer = preprocess_text(item.get('LONG_ANSWER', ''))\n",
        "\n",
        "        processed_data[key] = {\n",
        "            \"QUESTION\": question,\n",
        "            \"CONTEXTS\": context,\n",
        "            \"DECISION\": decision,\n",
        "            \"LONG_ANSWER\": long_answer,\n",
        "            \"YEAR\": item.get('YEAR', 'N/A'),\n",
        "            \"ORIGINAL_ID\": key\n",
        "        }\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(processed_data, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Processados {len(processed_data)} registros\")\n",
        "    return len(processed_data)\n",
        "\n",
        "# Processar dataset original completo\n",
        "total = preprocess_dataset(\n",
        "    'pubmedqa/data/ori_pqal.json',\n",
        "    'data_processed/ori_pqal_preprocessed.json'\n",
        ")\n",
        "\n",
        "print(f\"\\nPré-processamento concluído: {total} registros\")\n",
        "print(\"Arquivo salvo em: data_processed/ori_pqal_preprocessed.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ieOgONUhOkj"
      },
      "source": [
        "# Anonimização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xmlGwHjhWQJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def anonymize_text(text):\n",
        "    \"\"\"Remove dados sensíveis (LGPD/HIPAA compliance)\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'(Dr\\.|Dra\\.|Doctor|Prof\\.|MD)\\s+[A-Z][a-z]+(\\s+[A-Z][a-z]+)?', '[NOME]', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n",
        "    locations = r'(Israel|Denmark|Chile|Texas|France|United Kingdom|UK|USA|Pakistan|Karachi|Jordan|Japan|Australia|North Carolina|Washington)'\n",
        "    text = re.sub(locations, '[LOCAL]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b\\d{6,}\\b', '[ID]', text)\n",
        "    text = re.sub(r'\\b(19|20)\\d{2}\\b', '[ANO]', text)\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def anonymize_dataset(input_path, output_path):\n",
        "    \"\"\"Anonimiza o dataset pré-processado completo mantendo IDs e metadados\"\"\"\n",
        "    print(f\"Carregando {input_path}...\")\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    anonymized = {}\n",
        "\n",
        "    for key, item in data.items():\n",
        "        new_id = f\"HOSP_REG_{key[:8]}\"  #TODO: Revisar esse esquema de ID anonimizado\n",
        "\n",
        "        anonymized[new_id] = {\n",
        "            \"QUESTION\": anonymize_text(item.get('QUESTION', '')),\n",
        "            \"CONTEXTS\": anonymize_text(item.get('CONTEXTS', '')),\n",
        "            \"DECISION\": item.get('DECISION', ''),\n",
        "            \"LONG_ANSWER\": anonymize_text(item.get('LONG_ANSWER', '')),\n",
        "            \"YEAR\": item.get('YEAR', 'N/A'),\n",
        "            \"ORIGINAL_ID\": key,\n",
        "            \"SOURCE_DB\": \"PubMedQA\"\n",
        "        }\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(anonymized, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Anonimizados {len(anonymized)} registros\")\n",
        "    return len(anonymized)\n",
        "\n",
        "# Anonimizar dataset pré-processado\n",
        "total = anonymize_dataset(\n",
        "    'data_processed/ori_pqal_preprocessed.json',\n",
        "    'data_anonymized/ori_pqal_anonymized.json'\n",
        ")\n",
        "\n",
        "print(f\"\\nAnonimização concluída: {total} registros\")\n",
        "print(\"Arquivo salvo em: data_anonymized/ori_pqal_anonymized.json\")\n",
        "\n",
        "# Exemplo de dado anonimizado\n",
        "with open('data_anonymized/ori_pqal_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "    first_key = list(sample.keys())[0]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Exemplo de dado anonimizado:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ID: {first_key}\")\n",
        "    print(f\"Question: {sample[first_key]['QUESTION'][:100]}...\")\n",
        "    print(f\"Source: {sample[first_key]['SOURCE_DB']} | ID: {sample[first_key]['ORIGINAL_ID']} | Ano: {sample[first_key]['YEAR']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCr3ceRe-2iR"
      },
      "source": [
        "## Análise de Qualidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnLWXfQ1-5iu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_quality(data):\n",
        "    issues = {\n",
        "        'question_vazia': [],\n",
        "        'context_vazio': [],\n",
        "        'decision_vazia': [],\n",
        "        'context_muito_curto': []\n",
        "    }\n",
        "\n",
        "    for key, item in data.items():\n",
        "        if not item.get('QUESTION', '').strip():\n",
        "            issues['question_vazia'].append(key)\n",
        "\n",
        "        if not item.get('CONTEXTS', '').strip():\n",
        "            issues['context_vazio'].append(key)\n",
        "\n",
        "        if not item.get('DECISION', '').strip():\n",
        "            issues['decision_vazia'].append(key)\n",
        "\n",
        "        if len(item.get('CONTEXTS', '')) < 100:\n",
        "            issues['context_muito_curto'].append(key)\n",
        "\n",
        "    return issues\n",
        "\n",
        "def analyze_distribution(data):\n",
        "    distribution = Counter()\n",
        "\n",
        "    for key, item in data.items():\n",
        "        decision = item.get('DECISION', 'UNKNOWN')\n",
        "        distribution[decision] += 1\n",
        "\n",
        "    return distribution\n",
        "\n",
        "print(\"Analisando qualidade dos dados...\")\n",
        "\n",
        "with open('/content/data_anonymized/ori_pqal_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "issues = analyze_quality(test_data)\n",
        "\n",
        "print(\"\\nProblemas encontrados:\")\n",
        "for issue_type, ids in issues.items():\n",
        "    if ids:\n",
        "        print(f\"  {issue_type}: {len(ids)} registros\")\n",
        "    else:\n",
        "        print(f\"  {issue_type}: 0\")\n",
        "\n",
        "print(\"\\nDistribuição:\")\n",
        "dist_test = analyze_distribution(test_data)\n",
        "for cls, count in dist_test.items():\n",
        "    pct = (count / len(test_data)) * 100\n",
        "    print(f\"  {cls}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Total de registros: {len(test_data)}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9uXJezHAtls"
      },
      "source": [
        "## Fine Tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bl8is5mf6Jg"
      },
      "source": [
        "Instando as dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQgaM80e8ATp"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnhMX7WKhreU"
      },
      "source": [
        "Configuração das variáveis do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x34scJx7VTxP"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3K9nuvPmpzy"
      },
      "source": [
        "Conversão do dataset para treinamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "301QWFxfkUx7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "DATA_PATH = \"data_anonymized/ori_pqal_anonymized.json\"\n",
        "OUTPUT_DATA_PATH = \"data_final/final_pqal.json\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Você é um assistente médico-científico especializado. Sua função é responder perguntas médicas de forma clara e conversacional, como em um chat.\n",
        "\n",
        "REGRAS OBRIGATÓRIAS:\n",
        "- Responda de forma natural e conversacional, como se estivesse explicando para um colega\n",
        "- Base sua resposta EXCLUSIVAMENTE no contexto científico fornecido\n",
        "- Não utilize conhecimento externo\n",
        "- Seja direto e objetivo\n",
        "- Use linguagem clara e profissional\n",
        "- A resposta deve começar com SIM, NÃO ou TALVEZ, seguido de uma explicação natural\n",
        "\n",
        "CRITÉRIOS DE DECISÃO:\n",
        "- SIM: quando o contexto apoiar claramente a afirmação\n",
        "- NÃO: quando o contexto claramente contradizer a afirmação\n",
        "- TALVEZ: quando as evidências forem insuficientes, inconclusivas ou conflitantes\n",
        "\n",
        "PROIBIÇÕES:\n",
        "- Não inclua rótulos técnicos, marcadores ou formatação especial\n",
        "- Não repita a pergunta\n",
        "- Não faça recomendações de tratamentos ou medicamentos\n",
        "- Não use listas ou tópicos\n",
        "- Não mencione \"contexto\", \"análise\" ou termos técnicos de processamento\n",
        "\n",
        "Responda de forma fluida e natural, mantendo o tom profissional mas acessível.\n",
        "\"\"\"\n",
        "\n",
        "def validate_answer(decision):\n",
        "    \"\"\"Valida se a decisão está no formato correto\"\"\"\n",
        "    if decision in [\"SIM\", \"NÃO\", \"TALVEZ\"]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def format_response(decision: str, long_answer: str) -> str:\n",
        "    \"\"\"Formata resposta do assistente baseado na decisão - ELIMINA CÓDIGO DUPLICADO\"\"\"\n",
        "    prefixes = {\n",
        "        \"SIM\": \"Sim.\",\n",
        "        \"NÃO\": \"Não.\",\n",
        "        \"TALVEZ\": \"Talvez.\"\n",
        "    }\n",
        "    \n",
        "    defaults = {\n",
        "        \"SIM\": \"Baseado nas evidências disponíveis, a afirmação é correta.\",\n",
        "        \"NÃO\": \"Baseado nas evidências disponíveis, a afirmação não é correta.\",\n",
        "        \"TALVEZ\": \"As evidências disponíveis são inconclusivas ou insuficientes para uma resposta definitiva.\"\n",
        "    }\n",
        "    \n",
        "    prefix = prefixes.get(decision, \"\")\n",
        "    answer = long_answer if long_answer else defaults.get(decision, \"\")\n",
        "    \n",
        "    return f\"{prefix} {answer}\"\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "data = []\n",
        "for record_id, item in raw_data.items():\n",
        "    decision = item.get(\"DECISION\", \"\")\n",
        "    \n",
        "    if not validate_answer(decision):\n",
        "        continue\n",
        "\n",
        "    response = format_response(decision, item.get('LONG_ANSWER', ''))\n",
        "\n",
        "    # Adicionar informações de fonte no final da resposta\n",
        "    source_info = f\"\\n\\n[Fonte: {item.get('SOURCE_DB', 'N/A')} | ID: {item.get('ORIGINAL_ID', 'N/A')} | Ano: {item.get('YEAR', 'N/A')}]\"\n",
        "    response_with_source = response + source_info\n",
        "\n",
        "    data.append({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": item[\"QUESTION\"]},\n",
        "            {\"role\": \"assistant\", \"content\": response_with_source},\n",
        "        ],\n",
        "        \"metadata\": {\n",
        "            \"record_id\": record_id,\n",
        "            \"original_id\": item.get('ORIGINAL_ID', ''),\n",
        "            \"year\": item.get('YEAR', ''),\n",
        "            \"source_db\": item.get('SOURCE_DB', '')\n",
        "        }\n",
        "    })\n",
        "\n",
        "formatted_data = Dataset.from_list(data)\n",
        "\n",
        "print(\"Novo formato do dataset:\")\n",
        "print(json.dumps(formatted_data[0], indent=2, ensure_ascii=False))\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_DATA_PATH), exist_ok=True)\n",
        "with open(OUTPUT_DATA_PATH, 'w', encoding='utf-8') as output_file:\n",
        "    json.dump(formatted_data.to_list(), output_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dataset final salvo em: {OUTPUT_DATA_PATH}\")\n",
        "print(f\"Total de exemplos: {len(data)}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAuDSmVX3y7I"
      },
      "source": [
        "## Carregando o modelo \"unsloth/llama-3-8b-bnb-4bit\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiSOvZwN8Adm"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9xaYYui8AgY"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,  # AUMENTADO de 16 para 32 - maior capacidade de adaptação\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Mantém igual a r (recomendado)\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42MDAnh15Gwi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        full_text = \"\"\n",
        "        for msg in messages:\n",
        "            full_text += msg[\"content\"].strip() + \"\\n\"\n",
        "        texts.append(full_text.strip() + EOS_TOKEN)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "OUTPUT_PATH_DATASET = \"data_final/final_pqal.json\"\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=OUTPUT_PATH_DATASET,\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# SPLIT TRAIN/VALIDATION - 85% treino, 15% validação\n",
        "dataset_split = dataset.train_test_split(test_size=0.15, seed=42)\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(f\"Total de exemplos: {len(dataset)}\")\n",
        "print(f\"Treino: {len(train_dataset)} exemplos ({len(train_dataset)/len(dataset)*100:.1f}%)\")\n",
        "print(f\"Validação: {len(eval_dataset)} exemplos ({len(eval_dataset)/len(dataset)*100:.1f}%)\")\n",
        "print(\"\\nPrimeiro exemplo do dataset de treino:\")\n",
        "print(train_dataset[0][\"text\"][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uD7ptNq8ID6"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
        "os.environ[\"WANDB_PROJECT\"] = \"assistente-medico-lora\"  #TODO: validar porque por nome do projeto no WANDB\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  #TODO: validar porque salvar checkpoints no WANDB\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,  # Usando dataset de treino\n",
        "    eval_dataset = eval_dataset,    # Usando dataset de validação\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,  # TODO: Desejável aumentar para 500-1000 em produção\n",
        "        \n",
        "        # CONFIGURAÇÕES DE AVALIAÇÃO\n",
        "        eval_strategy = \"steps\",  # Avaliar a cada N steps\n",
        "        eval_steps = 20,          # Avaliar a cada 20 steps\n",
        "        \n",
        "        # CHECKPOINTING - salvar progresso\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 20,          # Salvar a cada 20 steps\n",
        "        save_total_limit = 3,     # Manter apenas os 3 melhores checkpoints\n",
        "        load_best_model_at_end = True,  # Carregar melhor modelo ao final\n",
        "        metric_for_best_model = \"eval_loss\",  # Métrica para escolher melhor modelo\n",
        "        \n",
        "        # LOGGING\n",
        "        logging_steps = 1,\n",
        "        logging_first_step = True,\n",
        "        report_to = \"wandb\",  # Enviar métricas para WANDB\n",
        "        \n",
        "        # Otimização\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rtcoC5nOohs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Carregar env\n",
        "ENV_PATH = \"/content/drive/MyDrive/token-hf/env\"\n",
        "load_dotenv(ENV_PATH)\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "HF_REPO = f\"{os.getenv(\"HF_USER_REPO\")}/assistente-medico-lora\"\n",
        "model.push_to_hub(HF_REPO)\n",
        "tokenizer.push_to_hub(HF_REPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt3Jqcl4SMF2"
      },
      "source": [
        "# TESTE BÁSICO DO MODELO TREINADO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06713958"
      },
      "outputs": [],
      "source": [
        "test_examples = [\n",
        "    {\n",
        "        \"question\": \"O uso diário de protetor solar previne o câncer de pele em pessoas com alto risco de desenvolver a doença?\",\n",
        "        \"context\": \"Estudos longitudinais mostraram que o uso regular e consistente de protetor solar com FPS 30 ou superior, em indivíduos com histórico familiar de melanoma ou com múltiplos nevos atípicos, reduziu significativamente a incidência de novos casos de câncer de pele não melanoma e melanoma em comparação com grupos de controle que usavam protetor solar ocasionalmente ou não usavam. A aplicação correta e reaplicação conforme as instruções são cruciais para a eficácia.\",\n",
        "        \"metadata\": {\n",
        "            \"source_db\": \"PubMedQA_Test\",\n",
        "            \"original_id\": \"TEST_001\",\n",
        "            \"year\": \"2024\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"A dieta cetogênica é recomendada como tratamento primário para hipertensão arterial em todos os pacientes?\",\n",
        "        \"context\": \"A dieta cetogênica tem mostrado resultados promissores na redução da pressão arterial em alguns estudos, especialmente em pacientes com obesidade e resistência à insulina. No entanto, sua segurança e eficácia a longo prazo como tratamento primário para hipertensão em toda a população de pacientes não foram estabelecidas por completo. Diretrizes atuais recomendam uma abordagem individualizada, considerando comorbidades e potencial para efeitos adversos. Alguns estudos indicam que, em pacientes específicos, pode ser uma opção viável sob supervisão médica, enquanto outros alertam para a necessidade de mais pesquisas antes de uma recomendação generalizada.\",\n",
        "        \"metadata\": {\n",
        "            \"source_db\": \"PubMedQA_Test\",\n",
        "            \"original_id\": \"TEST_002\",\n",
        "            \"year\": \"2024\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"A vacina contra a gripe é eficaz em 100% dos casos para prevenir a infecção pelo vírus influenza?\",\n",
        "        \"context\": \"A eficácia da vacina contra a gripe varia anualmente e depende de diversos fatores, como a correspondência entre as cepas da vacina e as que estão em circulação, e a idade e o estado de saúde do indivíduo vacinado. Em geral, a vacina é eficaz em 40% a 60% na prevenção da infecção pelo vírus influenza. Embora não seja 100% eficaz, ela reduz significativamente o risco de desenvolver a doença e suas complicações graves, incluindo hospitalizações e mortes.\",\n",
        "        \"metadata\": {\n",
        "            \"source_db\": \"PubMedQA_Test\",\n",
        "            \"original_id\": \"TEST_003\",\n",
        "            \"year\": \"2024\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"O consumo moderado de café está associado a um risco aumentado de doenças cardiovasculares em adultos saudáveis?\",\n",
        "        \"context\": \"Múltiplos estudos observacionais e meta-análises sugerem que o consumo moderado de café (cerca de 3-4 xícaras por dia) não está associado a um risco aumentado de doenças cardiovasculares em adultos saudáveis. Na verdade, alguns estudos indicam uma possível associação com um risco ligeiramente reduzido de certas condições cardíacas, embora mais pesquisas sejam necessárias para estabelecer causalidade. O consumo excessivo, no entanto, pode estar ligado a efeitos adversos em indivíduos sensíveis.\",\n",
        "        \"metadata\": {\n",
        "            \"source_db\": \"PubMedQA_Test\",\n",
        "            \"original_id\": \"TEST_004\",\n",
        "            \"year\": \"2024\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"O tratamento com antibióticos é sempre necessário para resfriados comuns?\",\n",
        "        \"context\": \"Resfriados comuns são infecções virais das vias aéreas superiores. Antibióticos são medicamentos projetados para combater infecções bacterianas e não têm efeito contra vírus. O uso desnecessário de antibióticos pode levar à resistência a antibióticos, um problema de saúde pública crescente. Portanto, o tratamento com antibióticos não é indicado para resfriados comuns.\",\n",
        "        \"metadata\": {\n",
        "            \"source_db\": \"PubMedQA_Test\",\n",
        "            \"original_id\": \"TEST_005\",\n",
        "            \"year\": \"2024\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Gerados {len(test_examples)} exemplos de teste.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7dc36457"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "eos_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"\\n--- Executando testes com exemplos gerados ---\")\n",
        "\n",
        "for i, example in enumerate(test_examples):\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "    metadata = example[\"metadata\"]\n",
        "\n",
        "    prompt = f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "    Pergunta:\n",
        "    {question}\n",
        "\n",
        "    Contexto científico:\n",
        "    {context}\n",
        "\n",
        "    Resposta:\n",
        "    \"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        eos_token_id=eos_id,\n",
        "        repetition_penalty=1.1,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    response = tokenizer.decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    # Adicionar fonte ao final da resposta\n",
        "    source_info = f\"\\n\\n[Fonte: {metadata['source_db']} | ID: {metadata['original_id']} | Ano: {metadata['year']}]\"\n",
        "    final_response = response + source_info\n",
        "\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(f\"PERGUNTA {i+1}:\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    print(f\"{question}\\n\")\n",
        "    \n",
        "    print(f\"{'─'*80}\")\n",
        "    print(f\"RESPOSTA DO ASSISTENTE:\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    print(final_response)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(\"\\nTestes concluídos.\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste com dados reais do dataset\n",
        "\n",
        "Teste usando exemplos reais do dataset anonimizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Carregar dados reais do dataset anonimizado\n",
        "DATA_FILE = '/content/data_anonymized/ori_pqal_anonymized.json'\n",
        "\n",
        "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    real_data = json.load(f)\n",
        "\n",
        "random_keys = random.sample(list(real_data.keys()), min(5, len(real_data)))\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"TESTE COM DADOS REAIS DO DATASET\")\n",
        "print(\"=\"*100)\n",
        "print(f\"\\nTotal de registros disponíveis: {len(real_data)}\")\n",
        "print(f\"Testando {len(random_keys)} exemplos aleatórios\\n\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "eos_id = tokenizer.eos_token_id\n",
        "\n",
        "for idx, key in enumerate(random_keys, 1):\n",
        "    item = real_data[key]\n",
        "    \n",
        "    question = item[\"QUESTION\"]\n",
        "    context = item[\"CONTEXTS\"]\n",
        "    expected_decision = item[\"DECISION\"]\n",
        "    metadata = {\n",
        "        \"source_db\": item.get(\"SOURCE_DB\", \"N/A\"),\n",
        "        \"original_id\": item.get(\"ORIGINAL_ID\", \"N/A\"),\n",
        "        \"year\": item.get(\"YEAR\", \"N/A\")\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "Contexto científico:\n",
        "{context}\n",
        "\n",
        "Pergunta:\n",
        "{question}\n",
        "\n",
        "Resposta:\n",
        "\"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        eos_token_id=eos_id,\n",
        "        repetition_penalty=1.1,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    model_response = tokenizer.decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    # Adicionar fonte\n",
        "    source_info = f\"\\n\\n[Fonte: {metadata['source_db']} | ID: {metadata['original_id']} | Ano: {metadata['year']}]\"\n",
        "    final_response = model_response + source_info\n",
        "\n",
        "    # Exibir resultado\n",
        "    print(\"\\n\" + \"─\"*100)\n",
        "    print(f\"TESTE {idx} | ID: {key}\")\n",
        "    print(\"─\"*100)\n",
        "    print(f\"\\nPERGUNTA:\")\n",
        "    print(f\"{question}\")\n",
        "    \n",
        "    print(f\"\\nCONTEXTO (primeiros 300 caracteres):\")\n",
        "    print(f\"{context[:300]}...\")\n",
        "    \n",
        "    print(f\"\\nDECISÃO ESPERADA (Ground Truth):\")\n",
        "    print(f\"{expected_decision}\")\n",
        "    \n",
        "    print(f\"\\nRESPOSTA DO MODELO:\")\n",
        "    print(final_response)\n",
        "    \n",
        "    # Verificar se a decisão está correta\n",
        "    model_decision = \"DESCONHECIDO\"\n",
        "    if model_response.upper().startswith(\"SIM\"):\n",
        "        model_decision = \"SIM\"\n",
        "    elif model_response.upper().startswith(\"NÃO\") or model_response.upper().startswith(\"NAO\"):\n",
        "        model_decision = \"NÃO\"\n",
        "    elif model_response.upper().startswith(\"TALVEZ\"):\n",
        "        model_decision = \"TALVEZ\"\n",
        "    \n",
        "    match = \"CORRETO\" if model_decision == expected_decision else \"INCORRETO\"\n",
        "    print(f\"\\nAVALIAÇÃO: {match}\")\n",
        "    print(f\"   Esperado: {expected_decision} | Modelo: {model_decision}\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "print(\"\\nTeste completo finalizado.\")\n",
        "print(\"=\"*100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aRFH2sIYarWe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
