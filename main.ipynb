{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszAxEbZOA43"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyGzkc5ez3a"
      },
      "source": [
        "# Download e exploração inicial dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CuxJtGggAxGf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pubmedqa/pubmedqa.git\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = 'pubmedqa/data/ori_pqal.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_key = list(data.keys())[0]\n",
        "print(f\"\\nCampos disponíveis: {list(data[sample_key].keys())}\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Exploração de dados - PubMedQA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, key in enumerate(list(data.keys())[:3]):\n",
        "    item = data[key]\n",
        "\n",
        "    print(f\"\\nExemplo {i+1} | ID: {key}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Question: {item.get('QUESTION', 'N/A')}\")\n",
        "\n",
        "    context = \" \".join(item.get('CONTEXTS', []))\n",
        "    print(f\"Context: {context[:300]}...\")\n",
        "\n",
        "    print(f\"Labels: {item.get('LABELS', 'N/A')}\")\n",
        "    print(f\"Decision: {item.get('final_decision', 'N/A')}\")\n",
        "    print(f\"Answer: {item.get('LONG_ANSWER', 'N/A')[:200]}...\")\n",
        "    print(f\"Meshes: {item.get('MESHES', 'N/A')}\")\n",
        "    print(f\"Year: {item.get('YEAR', 'N/A')}\")\n",
        "    print(f\"Reasoning required pred: {item.get('reasoning_required_pred', 'N/A')}\")\n",
        "    print(f\"Reasoning free pred: {item.get('reasoning_free_pred', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n\\nTotal de registros: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFH2sIYarWe"
      },
      "source": [
        "# Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4fd06e94"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Normaliza e limpa texto\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normalização unicode\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    # Normalização de espaços\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def map_decision(decision):\n",
        "    decision = decision.lower()\n",
        "    if decision == \"yes\":\n",
        "        return \"SIM\"\n",
        "    elif decision == \"no\":\n",
        "        return \"NÃO\"\n",
        "    elif decision == \"maybe\":\n",
        "        return \"TALVEZ\"\n",
        "    return\n",
        "\n",
        "def sanitize_answer(text):\n",
        "    forbiden_list = [\n",
        "        \"assistant.\",\n",
        "        \"assistant.Decision\",\n",
        "        \"assistant.Undefinitions\",\n",
        "        \"definitions\",\n",
        "        \"context\",\n",
        "        \"analysis\"\n",
        "    ]\n",
        "    for b in forbiden_list:\n",
        "        text = text.replace(b, \"\")\n",
        "    return text.strip()\n",
        "\n",
        "def preprocess_dataset(input_path, output_path):\n",
        "    \"\"\"Pré-processa o dataset original completo\"\"\"\n",
        "    print(f\"Carregando {input_path}...\")\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    for key, item in data.items():\n",
        "\n",
        "        # QUESTION\n",
        "        question = preprocess_text(item.get('QUESTION', ''))\n",
        "\n",
        "        # CONTEXTS (string ou lista)\n",
        "        context_raw = \" \".join(item.get('CONTEXTS', []))\n",
        "        context = preprocess_text(context_raw)\n",
        "\n",
        "        # DECISION\n",
        "        decision_raw = item.get('final_decision', 'N/A')\n",
        "        decision = map_decision(decision_raw)\n",
        "\n",
        "        # LONG_ANSWER\n",
        "        long_answer = preprocess_text(item.get('LONG_ANSWER', ''))\n",
        "        long_answer = sanitize_answer(long_answer)\n",
        "\n",
        "        answer = f\"Decisão: {decision}\\nJustificativa: {long_answer}\"\n",
        "\n",
        "        processed_data[key] = {\n",
        "            \"QUESTION\": question,\n",
        "            \"CONTEXTS\": context,\n",
        "            \"FINAL_ANSWER\": answer,\n",
        "            \"YEAR\": item.get('YEAR', 'N/A')\n",
        "        }\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(processed_data, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✓ Processados {len(processed_data)} registros\")\n",
        "    return len(processed_data)\n",
        "\n",
        "# Processar dataset original completo\n",
        "total = preprocess_dataset(\n",
        "    'pubmedqa/data/ori_pqal.json',\n",
        "    'data_processed/ori_pqal_preprocessed.json'\n",
        ")\n",
        "\n",
        "print(f\"\\nPré-processamento concluído: {total} registros\")\n",
        "print(\"Arquivo salvo em: data_processed/ori_pqal_preprocessed.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ieOgONUhOkj"
      },
      "source": [
        "# Anonimização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xmlGwHjhWQJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def anonymize_text(text):\n",
        "    \"\"\"Remove dados sensíveis (LGPD/HIPAA compliance)\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'(Dr\\.|Dra\\.|Doctor|Prof\\.|MD)\\s+[A-Z][a-z]+(\\s+[A-Z][a-z]+)?', '[NOME]', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n",
        "    locations = r'(Israel|Denmark|Chile|Texas|France|United Kingdom|UK|USA|Pakistan|Karachi|Jordan|Japan|Australia|North Carolina|Washington)'\n",
        "    text = re.sub(locations, '[LOCAL]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b\\d{6,}\\b', '[ID]', text)\n",
        "    text = re.sub(r'\\b(19|20)\\d{2}\\b', '[ANO]', text)\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def anonymize_dataset(input_path, output_path):\n",
        "    \"\"\"Anonimiza o dataset pré-processado completo\"\"\"\n",
        "    print(f\"Carregando {input_path}...\")\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    anonymized = {}\n",
        "\n",
        "    for key, item in data.items():\n",
        "        new_id = f\"HOSP_REG_{key[:4]}\"\n",
        "\n",
        "        anonymized[new_id] = {\n",
        "            \"QUESTION\": anonymize_text(item.get('QUESTION', '')),\n",
        "            \"CONTEXTS\": anonymize_text(item.get('CONTEXTS', '')),\n",
        "            \"FINAL_ANSWER\": anonymize_text(item.get('FINAL_ANSWER', '')),\n",
        "            \"YEAR\": item.get('YEAR', 'N/A')\n",
        "        }\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(anonymized, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✓ Anonimizados {len(anonymized)} registros\")\n",
        "    return len(anonymized)\n",
        "\n",
        "# Anonimizar dataset pré-processado\n",
        "total = anonymize_dataset(\n",
        "    'data_processed/ori_pqal_preprocessed.json',\n",
        "    'data_anonymized/ori_pqal_anonymized.json'\n",
        ")\n",
        "\n",
        "print(f\"\\nAnonimização concluída: {total} registros\")\n",
        "print(\"Arquivo salvo em: data_anonymized/ori_pqal_anonymized.json\")\n",
        "\n",
        "# Exemplo de dado anonimizado\n",
        "with open('data_anonymized/ori_pqal_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "    first_key = list(sample.keys())[0]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Exemplo de dado anonimizado:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ID: {first_key}\")\n",
        "    print(f\"Question: {sample[first_key]['QUESTION'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise de Qualidade"
      ],
      "metadata": {
        "id": "kCr3ceRe-2iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_quality(data):\n",
        "    issues = {\n",
        "        'question_vazia': [],\n",
        "        'context_vazio': [],\n",
        "        'answer_vazia': [],\n",
        "        'answer_muito_curta': [],\n",
        "        'context_muito_curto': []\n",
        "    }\n",
        "\n",
        "    for key, item in data.items():\n",
        "        if not item.get('QUESTION', '').strip():\n",
        "            issues['question_vazia'].append(key)\n",
        "\n",
        "        if not item.get('CONTEXTS', '').strip():\n",
        "            issues['context_vazio'].append(key)\n",
        "\n",
        "        if not item.get('FINAL_ANSWER', '').strip():\n",
        "            issues['answer_vazia'].append(key)\n",
        "\n",
        "        if len(item.get('FINAL_ANSWER', '')) < 50:\n",
        "            issues['answer_muito_curta'].append(key)\n",
        "\n",
        "        if len(item.get('CONTEXTS', '')) < 100:\n",
        "            issues['context_muito_curto'].append(key)\n",
        "\n",
        "    return issues\n",
        "\n",
        "def extract_decision(answer):\n",
        "    if answer.startswith(\"Decisão: SIM\"):\n",
        "        return \"SIM\"\n",
        "    if answer.startswith(\"Decisão: NÃO\"):\n",
        "        return \"NÃO\"\n",
        "    if answer.startswith(\"Decisão: TALVEZ\"):\n",
        "        return \"TALVEZ\"\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "def analyze_distribution(data):\n",
        "    distribution = Counter()\n",
        "\n",
        "    for key, item in data.items():\n",
        "        decision = extract_decision(item.get('FINAL_ANSWER',''))\n",
        "        distribution[decision] += 1\n",
        "\n",
        "    return distribution\n",
        "\n",
        "print(\"Analisando qualidade dos dados...\")\n",
        "\n",
        "# Analisar test set\n",
        "with open('/content/data_anonymized/ori_pqal_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "issues = analyze_quality(test_data)\n",
        "\n",
        "print(\"\\nProblemas encontrados:\")\n",
        "for issue_type, ids in issues.items():\n",
        "    if ids:\n",
        "        print(f\"  {issue_type}: {len(ids)} registros\")\n",
        "    else:\n",
        "        print(f\"  {issue_type}: 0\")\n",
        "\n",
        "print(\"\\nDistribuição:\")\n",
        "dist_test = analyze_distribution(test_data)\n",
        "for cls, count in dist_test.items():\n",
        "    pct = (count / len(test_data)) * 100\n",
        "    print(f\"  {cls}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Total de registros: {len(test_data)}\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "wnLWXfQ1-5iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tunning"
      ],
      "metadata": {
        "id": "l9uXJezHAtls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instando as dependências"
      ],
      "metadata": {
        "id": "3Bl8is5mf6Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "cQgaM80e8ATp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuração das variáveis do modelo"
      ],
      "metadata": {
        "id": "YnhMX7WKhreU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "x34scJx7VTxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversão do dataset para treinamento\n"
      ],
      "metadata": {
        "id": "F3K9nuvPmpzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "DATA_PATH = \"data_anonymized/ori_pqal_anonymized.json\"\n",
        "OUTPUT_DATA_PATH = \"data_final/final_pqal.json\"\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Você é um assistente médico-científico. Responda exclusivamente em português.\n",
        "\n",
        "Responda EXCLUSIVAMENTE com base no contexto fornecido.\n",
        "Não utilize conhecimento externo.\n",
        "\n",
        "REGRAS OBRIGATÓRIAS:\n",
        "- A resposta DEVE conter APENAS duas linhas.\n",
        "- A primeira linha DEVE começar exatamente com:\n",
        "  \"Decisão: SIM\", \"Decisão: NÃO\" ou \"Decisão: TALVEZ\"\n",
        "- A segunda linha DEVE começar exatamente com:\n",
        "  \"Justificativa:\"\n",
        "\n",
        "PROIBIÇÕES ABSOLUTAS:\n",
        "- NÃO inclua rótulos internos, nomes técnicos, marcadores ou palavras como:\n",
        "  \"assistant.\", \"assistant.Decision\", \"assistant.Undefinitions\", \"definitions\", \"context\", \"analysis\".\n",
        "- NÃO inclua listas, títulos, explicações adicionais ou texto fora do formato exigido.\n",
        "- NÃO repita a pergunta.\n",
        "- NÃO inclua qualquer texto antes ou depois das duas linhas exigidas.\n",
        "- Não faça recomendação de tratamentos\n",
        "- Não faça recomendação de medicamentos\n",
        "\n",
        "Use:\n",
        "- SIM quando o contexto apoiar claramente a afirmação.\n",
        "- NÃO quando o contexto claramente a contradizer.\n",
        "- TALVEZ apenas quando as evidências forem insuficientes, inconclusivas ou conflitantes.\n",
        "\"\"\"\n",
        "\n",
        "def validate_answer(answer):\n",
        "    lines = answer.strip().splitlines()\n",
        "    if not lines[0].startswith(\"Decisão:\"):\n",
        "        return False\n",
        "    if not lines[1].startswith(\"Justificativa:\"):\n",
        "        return False\n",
        "    if not any(x in lines[0] for x in [\"SIM\", \"NÃO\", \"TALVEZ\"]):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "data = []\n",
        "for _, item in raw_data.items():\n",
        "  if not validate_answer(item[\"FINAL_ANSWER\"]):\n",
        "    continue\n",
        "\n",
        "  data.append({\n",
        "      \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "          {\"role\": \"user\", \"content\": item[\"QUESTION\"]},\n",
        "          {\"role\": \"assistant\", \"content\": item[\"FINAL_ANSWER\"]},\n",
        "      ]\n",
        "  })\n",
        "\n",
        "formatted_data = Dataset.from_list(data)\n",
        "\n",
        "print(\"Novo formato do dataset:\")\n",
        "print(json.dumps(formatted_data[0], indent=2, ensure_ascii=False))\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_DATA_PATH), exist_ok=True)\n",
        "with open(OUTPUT_DATA_PATH, 'w', encoding='utf-8') as output_file:\n",
        "  json.dump(formatted_data.to_list(), output_file, indent=4)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dataset final salvo em: {OUTPUT_DATA_PATH}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "301QWFxfkUx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando o modelo \"unsloth/llama-3-8b-bnb-4bit\""
      ],
      "metadata": {
        "id": "AAuDSmVX3y7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ],
      "metadata": {
        "id": "GiSOvZwN8Adm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "q9xaYYui8AgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        full_text = \"\"\n",
        "        for msg in messages:\n",
        "            full_text += msg[\"content\"].strip() + \"\\n\"\n",
        "        texts.append(full_text.strip() + EOS_TOKEN)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "OUTPUT_PATH_DATASET = \"data_final/final_pqal.json\"\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=OUTPUT_PATH_DATASET,\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Primeiro exemplo do dataset formatado:\")\n",
        "print(dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "42MDAnh15Gwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,                   #Desejável aumentar, porém demora muito\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "-uD7ptNq8ID6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Carregar env\n",
        "ENV_PATH = \"/content/drive/MyDrive/token-hf/env\"\n",
        "load_dotenv(ENV_PATH)\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "HF_REPO = f\"{os.getenv(\"HF_USER_REPO\")}/assistente-medico-lora\"\n",
        "model.push_to_hub(HF_REPO)\n",
        "tokenizer.push_to_hub(HF_REPO)"
      ],
      "metadata": {
        "id": "-rtcoC5nOohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTE BÁSICO DO MODELO TREINADO"
      ],
      "metadata": {
        "id": "mt3Jqcl4SMF2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06713958"
      },
      "source": [
        "test_examples = [\n",
        "    {\n",
        "        \"question\": \"O uso diário de protetor solar previne o câncer de pele em pessoas com alto risco de desenvolver a doença?\",\n",
        "        \"context\": \"Estudos longitudinais mostraram que o uso regular e consistente de protetor solar com FPS 30 ou superior, em indivíduos com histórico familiar de melanoma ou com múltiplos nevos atípicos, reduziu significativamente a incidência de novos casos de câncer de pele não melanoma e melanoma em comparação com grupos de controle que usavam protetor solar ocasionalmente ou não usavam. A aplicação correta e reaplicação conforme as instruções são cruciais para a eficácia.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"A dieta cetogênica é recomendada como tratamento primário para hipertensão arterial em todos os pacientes?\",\n",
        "        \"context\": \"A dieta cetogênica tem mostrado resultados promissores na redução da pressão arterial em alguns estudos, especialmente em pacientes com obesidade e resistência à insulina. No entanto, sua segurança e eficácia a longo prazo como tratamento primário para hipertensão em toda a população de pacientes não foram estabelecidas por completo. Diretrizes atuais recomendam uma abordagem individualizada, considerando comorbidades e potencial para efeitos adversos. Alguns estudos indicam que, em pacientes específicos, pode ser uma opção viável sob supervisão médica, enquanto outros alertam para a necessidade de mais pesquisas antes de uma recomendação generalizada.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"A vacina contra a gripe é eficaz em 100% dos casos para prevenir a infecção pelo vírus influenza?\",\n",
        "        \"context\": \"A eficácia da vacina contra a gripe varia anualmente e depende de diversos fatores, como a correspondência entre as cepas da vacina e as que estão em circulação, e a idade e o estado de saúde do indivíduo vacinado. Em geral, a vacina é eficaz em 40% a 60% na prevenção da infecção pelo vírus influenza. Embora não seja 100% eficaz, ela reduz significativamente o risco de desenvolver a doença e suas complicações graves, incluindo hospitalizações e mortes.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"O consumo moderado de café está associado a um risco aumentado de doenças cardiovasculares em adultos saudáveis?\",\n",
        "        \"context\": \"Múltiplos estudos observacionais e meta-análises sugerem que o consumo moderado de café (cerca de 3-4 xícaras por dia) não está associado a um risco aumentado de doenças cardiovasculares em adultos saudáveis. Na verdade, alguns estudos indicam uma possível associação com um risco ligeiramente reduzido de certas condições cardíacas, embora mais pesquisas sejam necessárias para estabelecer causalidade. O consumo excessivo, no entanto, pode estar ligado a efeitos adversos em indivíduos sensíveis.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"O tratamento com antibióticos é sempre necessário para resfriados comuns?\",\n",
        "        \"context\": \"Resfriados comuns são infecções virais das vias aéreas superiores. Antibióticos são medicamentos projetados para combater infecções bacterianas e não têm efeito contra vírus. O uso desnecessário de antibióticos pode levar à resistência a antibióticos, um problema de saúde pública crescente. Portanto, o tratamento com antibióticos não é indicado para resfriados comuns.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Gerados {len(test_examples)} exemplos de teste.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7dc36457"
      },
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "eos_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"\\n--- Executando testes com exemplos gerados ---\")\n",
        "\n",
        "for i, example in enumerate(test_examples):\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "\n",
        "    prompt = f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "    Pergunta:\n",
        "    {question}\n",
        "\n",
        "    Contexto científico:\n",
        "    {context}\n",
        "\n",
        "    Resposta:\n",
        "    \"\"\".strip()\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        eos_token_id=eos_id,\n",
        "        repetition_penalty=1.1,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    response = tokenizer.decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    print(f\"\\n--- Exemplo {i+1} ---\")\n",
        "    print(f\"Pergunta: {question}\")\n",
        "    print(\"\\n\")\n",
        "    print(\"Resposta do Modelo:\")\n",
        "    print(response)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"\\n--- Testes concluídos ---\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aRFH2sIYarWe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}