{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2kI_IpN__Pb1",
        "outputId": "fb2b1ad9-d36f-4b58-dfd7-b122ebce52a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: langgraph in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (1.0.5)\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (0.49.0)\n",
            "Requirement already satisfied: peft in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: trl in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain) (1.2.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain) (2.12.5)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from bitsandbytes) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from bitsandbytes) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: transformers in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (4.57.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (1.12.0)\n",
            "Requirement already satisfied: safetensors in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from trl) (4.4.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (3.20.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from datasets>=3.0.0->trl) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.5.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.12.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jefer\\downloads\\tech-challenge-3\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install langchain langgraph bitsandbytes peft trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyGzkc5ez3a"
      },
      "source": [
        "# Download da base e amostargem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CuxJtGggAxGf",
        "outputId": "a4f2aed2-e044-4c0e-cb39-def01c5c543c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Campos dispon√≠veis no JSON: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER']\n",
            "\n",
            "\n",
            "==================================================\n",
            "EXPLORA√á√ÉO DE DADOS M√âDICOS - PUBMEDQA\n",
            "==================================================\n",
            "EXEMPLO #1 | ID (PMID): 21645374\n",
            "------------------------------------------------------------\n",
            "QUESTION (Pergunta): Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
            "CONTEXTS (Contexto m√©dico): Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cel...\n",
            "LABELS (R√≥tulos do contexto): ['BACKGROUND', 'RESULTS']\n",
            "FINAL_DECISION (Decis√£o): yes\n",
            "LONG_ANSWER (Resposta detalhada): Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of our knowledge, this is the first report of mitochondria and chloroplasts moving on transvacuolar str...\n",
            "MESHES (Termos m√©dicos): ['Alismataceae', 'Apoptosis', 'Cell Differentiation', 'Mitochondria', 'Plant Leaves']\n",
            "YEAR (Ano de publica√ß√£o): 2011\n",
            "REASONING_REQUIRED_PRED: yes\n",
            "REASONING_FREE_PRED: yes\n",
            "------------------------------------------------------------\n",
            "================================================================\n",
            "EXEMPLO #2 | ID (PMID): 16418930\n",
            "------------------------------------------------------------\n",
            "QUESTION (Pergunta): Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
            "CONTEXTS (Contexto m√©dico): Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differenc...\n",
            "LABELS (R√≥tulos do contexto): ['BACKGROUND', 'PATIENTS AND METHODS', 'RESULTS']\n",
            "FINAL_DECISION (Decis√£o): no\n",
            "LONG_ANSWER (Resposta detalhada): Using the charts described, there was only a slight overestimation of visual acuity by the Snellen E compared to the Landolt C, even in strabismus amblyopia. Small differences in the lower visual acuity range have to be considered....\n",
            "MESHES (Termos m√©dicos): ['Adolescent', 'Adult', 'Aged', 'Aged, 80 and over', 'Amblyopia', 'Cataract', 'Child', 'Eye Diseases', 'Female', 'Humans', 'Male', 'Middle Aged', 'Reference Values', 'Refractive Errors', 'Reproducibility of Results', 'Retinal Diseases', 'Strabismus', 'Vision Tests', 'Visual Acuity']\n",
            "YEAR (Ano de publica√ß√£o): 2006\n",
            "REASONING_REQUIRED_PRED: no\n",
            "REASONING_FREE_PRED: no\n",
            "------------------------------------------------------------\n",
            "================================================================\n",
            "EXEMPLO #3 | ID (PMID): 9488747\n",
            "------------------------------------------------------------\n",
            "QUESTION (Pergunta): Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
            "CONTEXTS (Contexto m√©dico): Apparent life-threatening events in infants are a difficult and frequent problem in pediatric practice. The prognosis is uncertain because of risk of sudden infant death syndrome. Eight infants aged 2 to 15 months were admitted during a period of 6 years; they suffered from similar maladies in the b...\n",
            "LABELS (R√≥tulos do contexto): ['BACKGROUND', 'CASE REPORTS']\n",
            "FINAL_DECISION (Decis√£o): yes\n",
            "LONG_ANSWER (Resposta detalhada): \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria....\n",
            "MESHES (Termos m√©dicos): ['Baths', 'Histamine', 'Humans', 'Infant', 'Syncope', 'Urticaria', 'Water']\n",
            "YEAR (Ano de publica√ß√£o): 1997\n",
            "REASONING_REQUIRED_PRED: yes\n",
            "REASONING_FREE_PRED: yes\n",
            "------------------------------------------------------------\n",
            "================================================================\n",
            "\n",
            "‚úÖ Total de registros dispon√≠veis para an√°lise: 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'pubmedqa'...\n"
          ]
        }
      ],
      "source": [
        "# Clona o reposit√≥rio\n",
        "!git clone https://github.com/pubmedqa/pubmedqa.git\n",
        "\n",
        "# Importa json e exibe uma amostra dos dados\n",
        "import json\n",
        "\n",
        "# Caminho para o arquivo carregado\n",
        "file_path = 'pubmedqa/data/ori_pqal.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_key = list(data.keys())[0]\n",
        "print(\"\\n\")\n",
        "print(f\"Campos dispon√≠veis no JSON: {list(data[sample_key].keys())}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"==================================================\")\n",
        "print(\"EXPLORA√á√ÉO DE DADOS M√âDICOS - PUBMEDQA\")\n",
        "print(\"==================================================\")\n",
        "\n",
        "# Loop pelas 3 primeiras entradas mostrando TODOS os campos solicitados\n",
        "for i, key in enumerate(list(data.keys())[:3]):\n",
        "    item = data[key]\n",
        "\n",
        "    print(f\"EXEMPLO #{i+1} | ID (PMID): {key}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Campos Principais\n",
        "    print(f\"QUESTION (Pergunta): {item.get('QUESTION', 'N/A')}\")\n",
        "\n",
        "    # Tratamento do Contexto (Lista para String)\n",
        "    contexto_completo = \" \".join(item.get('CONTEXTS', []))\n",
        "    print(f\"CONTEXTS (Contexto m√©dico): {contexto_completo[:300]}...\")\n",
        "\n",
        "    # Labels e Decis√£o\n",
        "    print(f\"LABELS (R√≥tulos do contexto): {item.get('LABELS', 'N/A')}\")\n",
        "    print(f\"FINAL_DECISION (Decis√£o): {item.get('final_decision', 'N/A')}\")\n",
        "    print(f\"LONG_ANSWER (Resposta detalhada): {item.get('LONG_ANSWER', 'N/A')[:300]}...\")\n",
        "\n",
        "    # Metadados e Predi√ß√µes\n",
        "    print(f\"MESHES (Termos m√©dicos): {item.get('MESHES', 'N/A')}\")\n",
        "    print(f\"YEAR (Ano de publica√ß√£o): {item.get('YEAR', 'N/A')}\")\n",
        "\n",
        "    # Campos de Racioc√≠nio (√öteis para an√°lise de erro/valida√ß√£o)\n",
        "    print(f\"REASONING_REQUIRED_PRED: {item.get('reasoning_required_pred', 'N/A')}\")\n",
        "    print(f\"REASONING_FREE_PRED: {item.get('reasoning_free_pred', 'N/A')}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(\"=\" * 64)\n",
        "\n",
        "print(f\"\\n‚úÖ Total de registros dispon√≠veis para an√°lise: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN3SxdO2mmXH"
      },
      "source": [
        "# Split de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXB173BcmqjF",
        "outputId": "e47a0180-11d4-4a9a-9220-26df93e1832f"
      },
      "outputs": [],
      "source": [
        "!cd pubmedqa/preprocess && python split_dataset.py pqal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFH2sIYarWe"
      },
      "source": [
        "# Pr√©-processamento de dados m√©dicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4fd06e94",
        "outputId": "b29258b7-ee04-4a0c-963e-1fd25652298f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INICIANDO PR√â-PROCESSAMENTO DOS DADOS SPLIT\n",
            "============================================================\n",
            "\n",
            "[1/11] Processando test_set.json...\n",
            "‚úÖ Conclu√≠do: 500 registros processados\n",
            "\n",
            "[2/11] Processando fold 0 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[3/11] Processando fold 1 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[4/11] Processando fold 2 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[5/11] Processando fold 3 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[6/11] Processando fold 4 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[7/11] Processando fold 5 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[8/11] Processando fold 6 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[9/11] Processando fold 7 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[10/11] Processando fold 8 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "[11/11] Processando fold 9 (dev_set.json)...\n",
            "‚úÖ Conclu√≠do: 50 registros processados\n",
            "\n",
            "============================================================\n",
            "PR√â-PROCESSAMENTO FINALIZADO COM SUCESSO!\n",
            "============================================================\n",
            "Arquivos salvos em: data_processed/\n",
            "  - test_set_preprocessed.json\n",
            "  - pqal_fold0-9/dev_set_preprocessed.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "\n",
        "def executar_preprocessing(texto):\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normaliza caracteres Unicode (converte s√≠mbolos como \\u0394, \\u03a8 em texto plano compat√≠vel)\n",
        "    texto = unicodedata.normalize('NFKC', texto)\n",
        "\n",
        "    # Remove espa√ßos extras, quebras de linha e tabs\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    # Mant√©m letras, n√∫meros, espa√ßos e pontua√ß√µes cl√≠nicas essenciais (incluindo %, + e -)\n",
        "    # Ajustado para n√£o remover s√≠mbolos de f√≥rmulas m√©dicas importantes\n",
        "    texto = re.sub(r'[^\\w\\s.,?!():%\\-\\+]', '', texto)\n",
        "\n",
        "    return texto\n",
        "\n",
        "def processar_arquivo(input_path, output_path):\n",
        "    \"\"\"Processa um √∫nico arquivo JSON\"\"\"\n",
        "    # Carrega os dados brutos\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dados_limpos = {}\n",
        "\n",
        "    # Itera sobre os registros para padroniz√°-los\n",
        "    for key, item in data.items():\n",
        "        # Preprocessing da Pergunta (QUESTION)\n",
        "        pergunta_limpa = executar_preprocessing(item.get('QUESTION', ''))\n",
        "\n",
        "        # Preprocessing do Contexto (Unindo a lista de CONTEXTS em uma string √∫nica limpa)\n",
        "        contexto_bruto = \" \".join(item.get('CONTEXTS', []))\n",
        "        contexto_limpo = executar_preprocessing(contexto_bruto)\n",
        "\n",
        "        # Unifica√ß√£o e Preprocessing da Resposta (Decis√£o + Explica√ß√£o Detalhada)\n",
        "        decisao = item.get('final_decision', 'N/A').upper()\n",
        "        resposta_longa = item.get('LONG_ANSWER', '')\n",
        "        # Criamos uma resposta estruturada para o treinamento\n",
        "        resposta_unificada = executar_preprocessing(f\"Decis√£o: {decisao}. Justificativa: {resposta_longa}\")\n",
        "\n",
        "        # Armazena os dados limpos\n",
        "        dados_limpos[key] = {\n",
        "            \"QUESTION\": pergunta_limpa,\n",
        "            \"CONTEXTS\": contexto_limpo,\n",
        "            \"FINAL_ANSWER\": resposta_unificada,\n",
        "            \"YEAR\": item.get('YEAR', 'N/A')\n",
        "        }\n",
        "\n",
        "    # Salva o resultado do Preprocessing\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(dados_limpos, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    return len(dados_limpos)\n",
        "\n",
        "# Criar diret√≥rio de sa√≠da para dados processados\n",
        "os.makedirs('data_processed', exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INICIANDO PR√â-PROCESSAMENTO DOS DADOS SPLIT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Processar test_set.json\n",
        "print(\"\\n[1/11] Processando test_set.json...\")\n",
        "total = processar_arquivo(\n",
        "    'pubmedqa/data/test_set.json',\n",
        "    'data_processed/test_set_preprocessed.json'\n",
        ")\n",
        "print(f\"Conclu√≠do: {total} registros processados\")\n",
        "\n",
        "# Processar cada fold (dev_set.json)\n",
        "for i in range(10):\n",
        "    print(f\"\\n[{i+2}/11] Processando fold {i} (dev_set.json)...\")\n",
        "    \n",
        "    # Criar diret√≥rio do fold\n",
        "    os.makedirs(f'data_processed/pqal_fold{i}', exist_ok=True)\n",
        "    \n",
        "    total = processar_arquivo(\n",
        "        f'pubmedqa/data/pqal_fold{i}/dev_set.json',\n",
        "        f'data_processed/pqal_fold{i}/dev_set_preprocessed.json'\n",
        "    )\n",
        "    print(f\"Conclu√≠do: {total} registros processados\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PR√â-PROCESSAMENTO FINALIZADO COM SUCESSO!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Arquivos salvos em: data_processed/\")\n",
        "print(f\"  - test_set_preprocessed.json\")\n",
        "print(f\"  - pqal_fold0-9/dev_set_preprocessed.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ieOgONUhOkj"
      },
      "source": [
        "# Anonimiza√ß√£o de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xmlGwHjhWQJ",
        "outputId": "b3a01cb8-3b36-4f00-bfe2-c01234470f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INICIANDO ANONIMIZA√á√ÉO DOS DADOS\n",
            "============================================================\n",
            "\n",
            "[1/11] Anonimizando test_set_preprocessed.json...\n",
            "Conclu√≠do: 435 registros protegidos\n",
            "\n",
            "[2/11] Anonimizando fold 0...\n",
            "Conclu√≠do: 50 registros protegidos\n",
            "\n",
            "[3/11] Anonimizando fold 1...\n",
            "Conclu√≠do: 50 registros protegidos\n",
            "\n",
            "[4/11] Anonimizando fold 2...\n",
            "Conclu√≠do: 49 registros protegidos\n",
            "\n",
            "[5/11] Anonimizando fold 3...\n",
            "Conclu√≠do: 49 registros protegidos\n",
            "\n",
            "[6/11] Anonimizando fold 4...\n",
            "Conclu√≠do: 50 registros protegidos\n",
            "\n",
            "[7/11] Anonimizando fold 5...\n",
            "Conclu√≠do: 50 registros protegidos\n",
            "\n",
            "[8/11] Anonimizando fold 6...\n",
            "Conclu√≠do: 48 registros protegidos\n",
            "\n",
            "[9/11] Anonimizando fold 7...\n",
            "Conclu√≠do: 50 registros protegidos\n",
            "\n",
            "[10/11] Anonimizando fold 8...\n",
            "Conclu√≠do: 50 registros protegidos\n",
            "\n",
            "[11/11] Anonimizando fold 9...\n",
            "Conclu√≠do: 50 registros protegidos\n",
            "\n",
            "============================================================\n",
            "ANONIMIZA√á√ÉO FINALIZADA COM SUCESSO!\n",
            "============================================================\n",
            "Arquivos salvos em: data_anonymized/\n",
            "  - test_set_anonymized.json\n",
            "  - pqal_fold0-9/dev_set_anonymized.json\n",
            "\n",
            "============================================================\n",
            "AUDITORIA DE SEGURAN√áA (EXEMPLO)\n",
            "============================================================\n",
            "Novo ID: HOSP_REG_1237\n",
            "Question: Is anorectal endosonography valuable in dyschesia?...\n",
            "ID Original: [OCULTADO_POR_SEGURANCA]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def executar_anonimizacao(texto):\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Mascara refer√™ncias a profissionais (Dr, Dra, MD, etc) + Nome\n",
        "    texto = re.sub(r'(Dr\\.|Dra\\.|Doctor|Prof\\.|MD)\\s+[A-Z][a-z]+(\\s+[A-Z][a-z]+)?', '[NOME_PROFISSIONAL]', texto)\n",
        "\n",
        "    # Mascara e-mails\n",
        "    texto = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[CONTATO_EMAIL]', texto)\n",
        "\n",
        "    # Mascara localiza√ß√µes geogr√°ficas\n",
        "    locais_regex = r'(Israel|Denmark|Chile|Texas|France|United Kingdom|UK|USA|Pakistan|Karachi|Jordan|Japan|Australia|North Carolina|Washington)'\n",
        "    texto = re.sub(locais_regex, '[LOCALIZACAO_RESTRITA]', texto, flags=re.IGNORECASE)\n",
        "\n",
        "    # Mascara IDs longos (6+ d√≠gitos)\n",
        "    texto = re.sub(r'\\b\\d{6,}\\b', '[ID_RESTRITO]', texto)\n",
        "\n",
        "    # Mascara anos\n",
        "    texto = re.sub(r'\\b(19|20)\\d{2}\\b', '[ANO]', texto)\n",
        "\n",
        "    # Mascara URLs\n",
        "    texto = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL_RESTRITA]', texto)\n",
        "\n",
        "    return texto\n",
        "\n",
        "def anonimizar_arquivo(input_path, output_path):\n",
        "    \"\"\"Anonimiza um √∫nico arquivo JSON\"\"\"\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dados_anonimizados = {}\n",
        "\n",
        "    for key, item in data.items():\n",
        "        # Anonimiza o ID\n",
        "        novo_id = f\"HOSP_REG_{key[:4]}\"\n",
        "\n",
        "        dados_anonimizados[novo_id] = {\n",
        "            \"QUESTION\": executar_anonimizacao(item.get('QUESTION', '')),\n",
        "            \"CONTEXTS\": executar_anonimizacao(item.get('CONTEXTS', '')),\n",
        "            \"FINAL_ANSWER\": executar_anonimizacao(item.get('FINAL_ANSWER', '')),\n",
        "            \"ORIGINAL_ID\": \"[OCULTADO_POR_SEGURANCA]\"\n",
        "        }\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(dados_anonimizados, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    return len(dados_anonimizados)\n",
        "\n",
        "# Criar diret√≥rio de sa√≠da para dados anonimizados\n",
        "os.makedirs('data_anonymized', exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INICIANDO ANONIMIZA√á√ÉO DOS DADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Anonimizar test_set\n",
        "print(\"\\n[1/11] Anonimizando test_set_preprocessed.json...\")\n",
        "total = anonimizar_arquivo(\n",
        "    'data_processed/test_set_preprocessed.json',\n",
        "    'data_anonymized/test_set_anonymized.json'\n",
        ")\n",
        "print(f\"Conclu√≠do: {total} registros protegidos\")\n",
        "\n",
        "# 2. Anonimizar cada fold\n",
        "for i in range(10):\n",
        "    print(f\"\\n[{i+2}/11] Anonimizando fold {i}...\")\n",
        "    \n",
        "    # Criar diret√≥rio do fold\n",
        "    os.makedirs(f'data_anonymized/pqal_fold{i}', exist_ok=True)\n",
        "    \n",
        "    total = anonimizar_arquivo(\n",
        "        f'data_processed/pqal_fold{i}/dev_set_preprocessed.json',\n",
        "        f'data_anonymized/pqal_fold{i}/dev_set_anonymized.json'\n",
        "    )\n",
        "    print(f\"Conclu√≠do: {total} registros protegidos\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANONIMIZA√á√ÉO FINALIZADA COM SUCESSO!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Arquivos salvos em: data_anonymized/\")\n",
        "print(f\"  - test_set_anonymized.json\")\n",
        "print(f\"  - pqal_fold0-9/dev_set_anonymized.json\")\n",
        "\n",
        "# Demonstra√ß√£o de seguran√ßa\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"AUDITORIA DE SEGURAN√áA (EXEMPLO)\")\n",
        "print(\"=\" * 60)\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    amostra = json.load(f)\n",
        "    primeira_chave = list(amostra.keys())[0]\n",
        "    print(f\"Novo ID: {primeira_chave}\")\n",
        "    print(f\"Question: {amostra[primeira_chave]['QUESTION'][:100]}...\")\n",
        "    print(f\"ID Original: {amostra[primeira_chave]['ORIGINAL_ID']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Curadoria de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "AN√ÅLISE DE QUALIDADE DOS DADOS\n",
            "============================================================\n",
            "\n",
            "üìä Analisando test_set...\n",
            "\n",
            "Problemas encontrados:\n",
            "\n",
            "üìà Distribui√ß√£o de classes (test_set):\n",
            "  - YES: 238 (54.7%)\n",
            "  - NO: 176 (40.5%)\n",
            "  - MAYBE: 21 (4.8%)\n",
            "\n",
            "üìà Distribui√ß√£o de classes (todos os folds):\n",
            "  - YES: 244 (56.1%)\n",
            "  - NO: 176 (40.5%)\n",
            "  - MAYBE: 15 (3.4%)\n",
            "\n",
            "‚úÖ Total de registros analisados:\n",
            "  - Test set: 435\n",
            "  - Dev folds: 435\n",
            "  - Total: 870\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def analisar_qualidade_dados(data):\n",
        "    \"\"\"Analisa qualidade dos dados\"\"\"\n",
        "    problemas = {\n",
        "        'question_vazia': [],\n",
        "        'context_vazio': [],\n",
        "        'answer_vazia': [],\n",
        "        'answer_muito_curta': [],\n",
        "        'context_muito_curto': []\n",
        "    }\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        # Verifica campos vazios\n",
        "        if not item.get('QUESTION', '').strip():\n",
        "            problemas['question_vazia'].append(key)\n",
        "        \n",
        "        if not item.get('CONTEXTS', '').strip():\n",
        "            problemas['context_vazio'].append(key)\n",
        "            \n",
        "        if not item.get('FINAL_ANSWER', '').strip():\n",
        "            problemas['answer_vazia'].append(key)\n",
        "        \n",
        "        # Verifica tamanho m√≠nimo\n",
        "        if len(item.get('FINAL_ANSWER', '')) < 50:\n",
        "            problemas['answer_muito_curta'].append(key)\n",
        "            \n",
        "        if len(item.get('CONTEXTS', '')) < 100:\n",
        "            problemas['context_muito_curto'].append(key)\n",
        "    \n",
        "    return problemas\n",
        "\n",
        "def extrair_decisao(final_answer):\n",
        "    \"\"\"Extrai a decis√£o (YES/NO/MAYBE) da resposta\"\"\"\n",
        "    final_answer_upper = final_answer.upper()\n",
        "    if 'YES' in final_answer_upper or 'SIM' in final_answer_upper:\n",
        "        return 'YES'\n",
        "    elif 'NO' in final_answer_upper or 'N√ÉO' in final_answer_upper or 'NAO' in final_answer_upper:\n",
        "        return 'NO'\n",
        "    elif 'MAYBE' in final_answer_upper or 'TALVEZ' in final_answer_upper:\n",
        "        return 'MAYBE'\n",
        "    return 'UNKNOWN'\n",
        "\n",
        "def balancear_classes(data):\n",
        "    \"\"\"Analisa distribui√ß√£o de classes\"\"\"\n",
        "    distribuicao = Counter()\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        decisao = extrair_decisao(item.get('FINAL_ANSWER', ''))\n",
        "        distribuicao[decisao] += 1\n",
        "    \n",
        "    return distribuicao\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"AN√ÅLISE DE QUALIDADE DOS DADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Carregar dados anonimizados do test_set\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# An√°lise de qualidade\n",
        "print(\"\\nüìä Analisando test_set...\")\n",
        "problemas = analisar_qualidade_dados(test_data)\n",
        "\n",
        "print(\"\\nProblemas encontrados:\")\n",
        "for tipo, ids in problemas.items():\n",
        "    if ids:\n",
        "        print(f\"  - {tipo}: {len(ids)} registros\")\n",
        "\n",
        "# Balanceamento de classes\n",
        "print(\"\\nüìà Distribui√ß√£o de classes (test_set):\")\n",
        "distribuicao_test = balancear_classes(test_data)\n",
        "for classe, qtd in distribuicao_test.items():\n",
        "    porcentagem = (qtd / len(test_data)) * 100\n",
        "    print(f\"  - {classe}: {qtd} ({porcentagem:.1f}%)\")\n",
        "\n",
        "# Analisar todos os folds\n",
        "print(\"\\nüìà Distribui√ß√£o de classes (todos os folds):\")\n",
        "all_folds_data = {}\n",
        "for i in range(10):\n",
        "    with open(f'data_anonymized/pqal_fold{i}/dev_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "        fold_data = json.load(f)\n",
        "        all_folds_data.update(fold_data)\n",
        "\n",
        "distribuicao_folds = balancear_classes(all_folds_data)\n",
        "for classe, qtd in distribuicao_folds.items():\n",
        "    porcentagem = (qtd / len(all_folds_data)) * 100\n",
        "    print(f\"  - {classe}: {qtd} ({porcentagem:.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total de registros analisados:\")\n",
        "print(f\"  - Test set: {len(test_data)}\")\n",
        "print(f\"  - Dev folds: {len(all_folds_data)}\")\n",
        "print(f\"  - Total: {len(test_data) + len(all_folds_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "VALIDA√á√ÉO DE CONSIST√äNCIA\n",
            "============================================================\n",
            "\n",
            "üìã Registro: HOSP_REG_1237\n",
            "Pergunta: Is anorectal endosonography valuable in dyschesia?...\n",
            "Contexto (tamanho): 1231 caracteres\n",
            "Resposta: Decis√£o: YES. Justificativa: Linear anorectal endosonography demonstrated incomplete or even absent relaxation of the anal sphincter and the m. pubore...\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìã Registro: HOSP_REG_2616\n",
            "Pergunta: Is there a connection between sublingual varices and hypertension?...\n",
            "Contexto (tamanho): 1667 caracteres\n",
            "Resposta: Decis√£o: YES. Justificativa: An association was found between sublingual varices and hypertension. Examining the lateral borders of the tongue is easi...\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìã Registro: HOSP_REG_1910\n",
            "Pergunta: Are home sampling kits for sexually transmitted infections acceptable among men who have sex with me...\n",
            "Contexto (tamanho): 1224 caracteres\n",
            "Resposta: Decis√£o: MAYBE. Justificativa: The widespread acceptability of using HSKs for the diagnosis of STIs could have important public health impacts in term...\n",
            "------------------------------------------------------------\n",
            "\n",
            "‚úÖ Todas as amostras validadas est√£o consistentes\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def validar_consistencia(data, max_amostras=5):\n",
        "    \"\"\"Valida consist√™ncia entre pergunta, contexto e resposta\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"VALIDA√á√ÉO DE CONSIST√äNCIA\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    inconsistencias = []\n",
        "    \n",
        "    for key, item in list(data.items())[:max_amostras]:\n",
        "        question = item.get('QUESTION', '')\n",
        "        context = item.get('CONTEXTS', '')\n",
        "        answer = item.get('FINAL_ANSWER', '')\n",
        "        \n",
        "        # Verifica√ß√µes simples de consist√™ncia\n",
        "        tem_interrogacao = '?' in question\n",
        "        context_relevante = len(context) > 50\n",
        "        answer_fundamentada = len(answer) > 30\n",
        "        \n",
        "        if not (tem_interrogacao and context_relevante and answer_fundamentada):\n",
        "            inconsistencias.append({\n",
        "                'id': key,\n",
        "                'tem_interrogacao': tem_interrogacao,\n",
        "                'context_relevante': context_relevante,\n",
        "                'answer_fundamentada': answer_fundamentada\n",
        "            })\n",
        "        \n",
        "        # Exibir amostra\n",
        "        print(f\"\\nüìã Registro: {key}\")\n",
        "        print(f\"Pergunta: {question[:100]}...\")\n",
        "        print(f\"Contexto (tamanho): {len(context)} caracteres\")\n",
        "        print(f\"Resposta: {answer[:150]}...\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    return inconsistencias\n",
        "\n",
        "# Validar amostra do test_set\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "inconsistencias = validar_consistencia(test_data, max_amostras=3)\n",
        "\n",
        "if inconsistencias:\n",
        "    print(f\"\\n‚ö†Ô∏è Encontradas {len(inconsistencias)} inconsist√™ncias nas amostras\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Todas as amostras validadas est√£o consistentes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FORMATA√á√ÉO PARA FINE-TUNING\n",
            "============================================================\n",
            "\n",
            "[1/11] Formatando test_set...\n",
            "‚úÖ 435 exemplos formatados\n",
            "\n",
            "[2/11] Formatando fold 0...\n",
            "‚úÖ 50 exemplos formatados\n",
            "\n",
            "[3/11] Formatando fold 1...\n",
            "‚úÖ 50 exemplos formatados\n",
            "\n",
            "[4/11] Formatando fold 2...\n",
            "‚úÖ 49 exemplos formatados\n",
            "\n",
            "[5/11] Formatando fold 3...\n",
            "‚úÖ 49 exemplos formatados\n",
            "\n",
            "[6/11] Formatando fold 4...\n",
            "‚úÖ 50 exemplos formatados\n",
            "\n",
            "[7/11] Formatando fold 5...\n",
            "‚úÖ 50 exemplos formatados\n",
            "\n",
            "[8/11] Formatando fold 6...\n",
            "‚úÖ 48 exemplos formatados\n",
            "\n",
            "[9/11] Formatando fold 7...\n",
            "‚úÖ 50 exemplos formatados\n",
            "\n",
            "[10/11] Formatando fold 8...\n",
            "‚úÖ 50 exemplos formatados\n",
            "\n",
            "[11/11] Formatando fold 9...\n",
            "‚úÖ 50 exemplos formatados\n",
            "\n",
            "============================================================\n",
            "FORMATA√á√ÉO CONCLU√çDA!\n",
            "============================================================\n",
            "Formato: JSONL conversacional (compat√≠vel com LLaMA, Falcon, Mistral)\n",
            "Estrutura: system + user (contexto + pergunta) + assistant (resposta)\n",
            "\n",
            "Arquivos salvos em: data_curated/\n",
            "\n",
            "============================================================\n",
            "EXEMPLO DE DADO FORMATADO\n",
            "============================================================\n",
            "{\n",
            "  \"id\": \"HOSP_REG_1237\",\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"system\",\n",
            "      \"content\": \"Voc√™ √© um assistente m√©dico especializado. Responda √†s perguntas baseando-se nas evid√™ncias cient√≠ficas fornecidas no contexto.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"Contexto m√©dico: Dyschesia can be provoked by inappropriate defecation movements. The aim of this prospective study was to demonstrate dysfunction of the anal sphincter andor the musculus (m.) puborectalis in patients with dyschesia using anorectal endosonography. Twenty consecutive patients with a medical history of dyschesia and a control group of 20 healthy subjects underwent linear anorectal endosonography (Toshiba models IUV 5060 and PVL-625 RT). In both groups, the dimensions of the anal sphincter and the m. puborec...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def formatar_para_finetuning(data, output_path, formato='conversational'):\n",
        "    \"\"\"\n",
        "    Formata dados para fine-tuning no padr√£o conversacional\n",
        "    Compat√≠vel com LLaMA, Falcon, Mistral, etc.\n",
        "    \"\"\"\n",
        "    dados_formatados = []\n",
        "    \n",
        "    for key, item in data.items():\n",
        "        question = item.get('QUESTION', '')\n",
        "        context = item.get('CONTEXTS', '')\n",
        "        answer = item.get('FINAL_ANSWER', '')\n",
        "        \n",
        "        # Formato conversacional (formato Alpaca/ChatML)\n",
        "        exemplo = {\n",
        "            \"id\": key,\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"Voc√™ √© um assistente m√©dico especializado. Responda √†s perguntas baseando-se nas evid√™ncias cient√≠ficas fornecidas no contexto.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Contexto m√©dico: {context}\\n\\nPergunta: {question}\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": answer\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        dados_formatados.append(exemplo)\n",
        "    \n",
        "    # Salvar em formato JSONL (linha por linha)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for item in dados_formatados:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    \n",
        "    return len(dados_formatados)\n",
        "\n",
        "# Criar diret√≥rio para dados curados\n",
        "os.makedirs('data_curated', exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FORMATA√á√ÉO PARA FINE-TUNING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Formatar test_set\n",
        "print(\"\\n[1/11] Formatando test_set...\")\n",
        "with open('data_anonymized/test_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "total = formatar_para_finetuning(\n",
        "    test_data,\n",
        "    'data_curated/test_set_curated.jsonl'\n",
        ")\n",
        "print(f\"{total} exemplos formatados\")\n",
        "\n",
        "# Formatar cada fold\n",
        "for i in range(10):\n",
        "    print(f\"\\n[{i+2}/11] Formatando fold {i}...\")\n",
        "    \n",
        "    os.makedirs(f'data_curated/pqal_fold{i}', exist_ok=True)\n",
        "    \n",
        "    with open(f'data_anonymized/pqal_fold{i}/dev_set_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "        fold_data = json.load(f)\n",
        "    \n",
        "    total = formatar_para_finetuning(\n",
        "        fold_data,\n",
        "        f'data_curated/pqal_fold{i}/dev_set_curated.jsonl'\n",
        "    )\n",
        "    print(f\"{total} exemplos formatados\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FORMATA√á√ÉO CONCLU√çDA!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Formato: JSONL conversacional (compat√≠vel com LLaMA, Falcon, Mistral)\")\n",
        "print(\"Estrutura: system + user (contexto + pergunta) + assistant (resposta)\")\n",
        "print(f\"\\nArquivos salvos em: data_curated/\")\n",
        "\n",
        "# Exibir exemplo formatado\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXEMPLO DE DADO FORMATADO\")\n",
        "print(\"=\" * 60)\n",
        "with open('data_curated/test_set_curated.jsonl', 'r', encoding='utf-8') as f:\n",
        "    exemplo = json.loads(f.readline())\n",
        "    print(json.dumps(exemplo, indent=2, ensure_ascii=False)[:800] + \"...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aRFH2sIYarWe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
