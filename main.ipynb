{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszAxEbZOA43"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WyGzkc5ez3a"
      },
      "source": [
        "# Download e exploração inicial dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CuxJtGggAxGf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pubmedqa/pubmedqa.git\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = 'pubmedqa/data/ori_pqal.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_key = list(data.keys())[0]\n",
        "print(f\"\\nCampos disponíveis: {list(data[sample_key].keys())}\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Exploração de dados - PubMedQA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, key in enumerate(list(data.keys())[:3]):\n",
        "    item = data[key]\n",
        "\n",
        "    print(f\"\\nExemplo {i+1} | ID: {key}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Question: {item.get('QUESTION', 'N/A')}\")\n",
        "\n",
        "    context = \" \".join(item.get('CONTEXTS', []))\n",
        "    print(f\"Context: {context[:300]}...\")\n",
        "\n",
        "    print(f\"Labels: {item.get('LABELS', 'N/A')}\")\n",
        "    print(f\"Decision: {item.get('final_decision', 'N/A')}\")\n",
        "    print(f\"Answer: {item.get('LONG_ANSWER', 'N/A')[:200]}...\")\n",
        "    print(f\"Meshes: {item.get('MESHES', 'N/A')}\")\n",
        "    print(f\"Year: {item.get('YEAR', 'N/A')}\")\n",
        "    print(f\"Reasoning required pred: {item.get('reasoning_required_pred', 'N/A')}\")\n",
        "    print(f\"Reasoning free pred: {item.get('reasoning_free_pred', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n\\nTotal de registros: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFH2sIYarWe"
      },
      "source": [
        "# Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4fd06e94"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Normaliza e limpa texto\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normalização unicode\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    # Normalização de espaços\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_dataset(input_path, output_path):\n",
        "    \"\"\"Pré-processa o dataset original completo\"\"\"\n",
        "    print(f\"Carregando {input_path}...\")\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    for key, item in data.items():\n",
        "\n",
        "        # QUESTION\n",
        "        question = preprocess_text(item.get('QUESTION', ''))\n",
        "\n",
        "        # CONTEXTS (string ou lista)\n",
        "        context_raw = \" \".join(item.get('CONTEXTS', []))\n",
        "        context = preprocess_text(context_raw)\n",
        "\n",
        "        decision = preprocess_text(item.get('final_decision', 'N/A').upper())\n",
        "        long_answer = preprocess_text(item.get('LONG_ANSWER', ''))\n",
        "        answer = f\"Decisão: {decision}\\nJustificativa:\\n{long_answer}\"\n",
        "\n",
        "        processed_data[key] = {\n",
        "            \"QUESTION\": question,\n",
        "            \"CONTEXTS\": context,\n",
        "            \"FINAL_ANSWER\": answer,\n",
        "            \"YEAR\": item.get('YEAR', 'N/A')\n",
        "        }\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(processed_data, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✓ Processados {len(processed_data)} registros\")\n",
        "    return len(processed_data)\n",
        "\n",
        "# Processar dataset original completo\n",
        "total = preprocess_dataset(\n",
        "    'pubmedqa/data/ori_pqal.json',\n",
        "    'data_processed/ori_pqal_preprocessed.json'\n",
        ")\n",
        "\n",
        "print(f\"\\nPré-processamento concluído: {total} registros\")\n",
        "print(\"Arquivo salvo em: data_processed/ori_pqal_preprocessed.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ieOgONUhOkj"
      },
      "source": [
        "# Anonimização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xmlGwHjhWQJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def anonymize_text(text):\n",
        "    \"\"\"Remove dados sensíveis (LGPD/HIPAA compliance)\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'(Dr\\.|Dra\\.|Doctor|Prof\\.|MD)\\s+[A-Z][a-z]+(\\s+[A-Z][a-z]+)?', '[NOME]', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n",
        "    locations = r'(Israel|Denmark|Chile|Texas|France|United Kingdom|UK|USA|Pakistan|Karachi|Jordan|Japan|Australia|North Carolina|Washington)'\n",
        "    text = re.sub(locations, '[LOCAL]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b\\d{6,}\\b', '[ID]', text)\n",
        "    text = re.sub(r'\\b(19|20)\\d{2}\\b', '[ANO]', text)\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def anonymize_dataset(input_path, output_path):\n",
        "    \"\"\"Anonimiza o dataset pré-processado completo\"\"\"\n",
        "    print(f\"Carregando {input_path}...\")\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    anonymized = {}\n",
        "\n",
        "    for key, item in data.items():\n",
        "        new_id = f\"HOSP_REG_{key[:4]}\"\n",
        "\n",
        "        anonymized[new_id] = {\n",
        "            \"QUESTION\": anonymize_text(item.get('QUESTION', '')),\n",
        "            \"CONTEXTS\": anonymize_text(item.get('CONTEXTS', '')),\n",
        "            \"FINAL_ANSWER\": anonymize_text(item.get('FINAL_ANSWER', '')),\n",
        "            \"YEAR\": item.get('YEAR', 'N/A')\n",
        "        }\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        json.dump(anonymized, f_out, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✓ Anonimizados {len(anonymized)} registros\")\n",
        "    return len(anonymized)\n",
        "\n",
        "# Anonimizar dataset pré-processado\n",
        "total = anonymize_dataset(\n",
        "    'data_processed/ori_pqal_preprocessed.json',\n",
        "    'data_anonymized/ori_pqal_anonymized.json'\n",
        ")\n",
        "\n",
        "print(f\"\\nAnonimização concluída: {total} registros\")\n",
        "print(\"Arquivo salvo em: data_anonymized/ori_pqal_anonymized.json\")\n",
        "\n",
        "# Exemplo de dado anonimizado\n",
        "with open('data_anonymized/ori_pqal_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "    first_key = list(sample.keys())[0]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Exemplo de dado anonimizado:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"ID: {first_key}\")\n",
        "    print(f\"Question: {sample[first_key]['QUESTION'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise de Qualidade"
      ],
      "metadata": {
        "id": "kCr3ceRe-2iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_quality(data):\n",
        "    issues = {\n",
        "        'question_vazia': [],\n",
        "        'context_vazio': [],\n",
        "        'answer_vazia': [],\n",
        "        'answer_muito_curta': [],\n",
        "        'context_muito_curto': []\n",
        "    }\n",
        "\n",
        "    for key, item in data.items():\n",
        "        if not item.get('QUESTION', '').strip():\n",
        "            issues['question_vazia'].append(key)\n",
        "\n",
        "        if not item.get('CONTEXTS', '').strip():\n",
        "            issues['context_vazio'].append(key)\n",
        "\n",
        "        if not item.get('FINAL_ANSWER', '').strip():\n",
        "            issues['answer_vazia'].append(key)\n",
        "\n",
        "        if len(item.get('FINAL_ANSWER', '')) < 50:\n",
        "            issues['answer_muito_curta'].append(key)\n",
        "\n",
        "        if len(item.get('CONTEXTS', '')) < 100:\n",
        "            issues['context_muito_curto'].append(key)\n",
        "\n",
        "    return issues\n",
        "\n",
        "def extract_decision(answer):\n",
        "    answer_upper = answer.upper()\n",
        "    if 'YES' in answer_upper or 'SIM' in answer_upper:\n",
        "        return 'YES'\n",
        "    elif 'NO' in answer_upper or 'NÃO' in answer_upper or 'NAO' in answer_upper:\n",
        "        return 'NO'\n",
        "    elif 'MAYBE' in answer_upper or 'TALVEZ' in answer_upper:\n",
        "        return 'MAYBE'\n",
        "    return 'UNKNOWN'\n",
        "\n",
        "def analyze_distribution(data):\n",
        "    distribution = Counter()\n",
        "\n",
        "    for key, item in data.items():\n",
        "        decision = extract_decision(item.get('FINAL_ANSWER',''))\n",
        "        distribution[decision] += 1\n",
        "\n",
        "    return distribution\n",
        "\n",
        "print(\"Analisando qualidade dos dados...\")\n",
        "\n",
        "# Analisar test set\n",
        "with open('/content/data_anonymized/ori_pqal_anonymized.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "issues = analyze_quality(test_data)\n",
        "\n",
        "print(\"\\nProblemas encontrados:\")\n",
        "for issue_type, ids in issues.items():\n",
        "    if ids:\n",
        "        print(f\"  {issue_type}: {len(ids)} registros\")\n",
        "    else:\n",
        "        print(f\"  {issue_type}: 0\")\n",
        "\n",
        "print(\"\\nDistribuição:\")\n",
        "dist_test = analyze_distribution(test_data)\n",
        "for cls, count in dist_test.items():\n",
        "    pct = (count / len(test_data)) * 100\n",
        "    print(f\"  {cls}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Total de registros: {len(test_data)}\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {
        "id": "wnLWXfQ1-5iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tunning"
      ],
      "metadata": {
        "id": "l9uXJezHAtls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instando as dependências"
      ],
      "metadata": {
        "id": "3Bl8is5mf6Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "cQgaM80e8ATp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuração das variáveis do modelo"
      ],
      "metadata": {
        "id": "YnhMX7WKhreU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "x34scJx7VTxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversão do dataset para treinamento\n"
      ],
      "metadata": {
        "id": "F3K9nuvPmpzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "DATA_PATH = \"data_anonymized/ori_pqal_anonymized.json\"\n",
        "OUTPUT_DATA_PATH = \"data_final/final_pqal.json\"\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Você é um assistente médico-científico.\n",
        "\n",
        "Responda exclusivamente com base no contexto fornecido.\n",
        "\n",
        "Formato obrigatório da resposta:\n",
        "Decisão: YES | NO | MAYBE\n",
        "Justificativa: <explicação objetiva>\n",
        "\n",
        "Use:\n",
        "- YES quando o contexto apoiar claramente a afirmação.\n",
        "- NO quando o contexto claramente a contradizer.\n",
        "- Evite usar MAYBE, use apenas quando as evidências forem insuficientes, inconclusivas ou conflitantes.\n",
        "\n",
        "Não use conhecimento externo.\n",
        "\"\"\"\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "data = []\n",
        "for _, item in raw_data.items():\n",
        "    data.append({\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    f\"{SYSTEM_PROMPT}\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"Pergunta:\\n{item['QUESTION']}\\n\\n\"\n",
        "                    f\"Contexto científico:\\n{item['CONTEXTS']}\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": item[\"FINAL_ANSWER\"]\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "formatted_data = Dataset.from_list(data)\n",
        "\n",
        "print(\"Novo formato do dataset:\")\n",
        "print(json.dumps(formatted_data[0], indent=2, ensure_ascii=False))\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_DATA_PATH), exist_ok=True)\n",
        "with open(OUTPUT_DATA_PATH, 'w', encoding='utf-8') as output_file:\n",
        "  json.dump(formatted_data.to_list(), output_file, indent=4)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dataset final salvo em: {OUTPUT_DATA_PATH}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "301QWFxfkUx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando o modelo \"unsloth/llama-3-8b-bnb-4bit\""
      ],
      "metadata": {
        "id": "AAuDSmVX3y7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ],
      "metadata": {
        "id": "GiSOvZwN8Adm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "q9xaYYui8AgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Define o EOS_TOKEN que já foi carregado com o tokenizer\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Explicitly set the chat template for Llama-3 to prevent ValueError\n",
        "# This template is based on the official Llama 3 chat format\n",
        "tokenizer.chat_template = (\n",
        "    \"{% for message in messages %}\"\n",
        "        \"{% if message['role'] == 'system' %}\"\n",
        "            \"<|start_header_id|>system<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>\"\n",
        "        \"{% elif message['role'] == 'user' %}\"\n",
        "            \"<|start_header_id|>user<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>\"\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\n",
        "            \"<|start_header_id|>assistant<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>\"\n",
        "        \"{% endif %}\"\n",
        "    \"{% endfor %}\"\n",
        ")\n",
        "\n",
        "\n",
        "# Função de formatação adaptada para Llama 3 chat\n",
        "def formatting_prompts_func(examples):\n",
        "    # Aplica o chat template do tokenizer diretamente aos 'messages'\n",
        "    # O formatted_data de `301QWFxfkUx7` já está no formato de lista de dicionários com 'role' e 'content'\n",
        "    texts = [tokenizer.apply_chat_template(messages_list, tokenize=False, add_generation_prompt=False) + EOS_TOKEN for messages_list in examples[\"messages\"]]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Caminho para o dataset pré-processado e anonimizado\n",
        "OUTPUT_PATH_DATASET = \"data_final/final_pqal.json\"\n",
        "\n",
        "# Carrega o dataset\n",
        "dataset = load_dataset(\"json\", data_files=OUTPUT_PATH_DATASET, split = \"train\")\n",
        "\n",
        "# Aplica a função de formatação ao dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "print(\"Primeiro exemplo do dataset formatado para o Fine-tuning:\")\n",
        "print(dataset[0][\"text\"])\n"
      ],
      "metadata": {
        "id": "42MDAnh15Gwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "-uD7ptNq8ID6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aRFH2sIYarWe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}